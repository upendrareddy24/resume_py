# Simplification Plan for resume_py

## ğŸ¯ Goals
1. Make it work reliably like AIHawk
2. Remove unnecessary complexity
3. Keep the best features (parallel processing, multi-company support)
4. Easier to debug and maintain

## ğŸ”§ Step-by-Step Simplification

### Phase 1: Remove Optional Dependencies âœ… (High Priority)

#### 1.1 Make Embeddings Optional
**Current Issue:** Requires FAISS, vector stores, embeddings
**Solution:** Use direct LLM extraction only

```python
# OLD (Complex)
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
vectorstore = FAISS.from_texts(chunks, embeddings)

# NEW (Simple)
from openai import OpenAI
client = OpenAI(api_key=api_key)
response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": prompt}]
)
```

**Files to modify:**
- `llm_parser_adapter.py` â†’ Make it optional
- `llm_job_parser.py` â†’ Remove embedding requirement
- `match.py` â†’ Skip LLMParser if embeddings unavailable

#### 1.2 Simplify LangChain Usage
**Current:** Uses LangChain for everything
**New:** Use LangChain only where needed, direct OpenAI elsewhere

**Benefits:**
- Fewer dependencies
- Faster execution
- Easier to debug

---

### Phase 2: Simplify Configuration ğŸ“

#### 2.1 Convert to YAML (like AIHawk)
**Current:** Complex JSON with nested objects
**New:** Simple YAML

```yaml
# config.yaml (NEW - Simple!)
resume: input/resume.txt
openai_key: ${OPENAI_API_KEY}

job_search:
  companies:
    - uber
    - apple
    - meta
    - google
    - amazon
    - microsoft
    - netflix
  
  filters:
    min_score: 50
    fetch_limit: 10
    locations:
      - united states
  
  parallel_workers: 5
  top_per_company: true

generation:
  auto_tailor: true
  tailor_threshold: 50
  generate_cover_letter: true
  generate_summary: true
```

**vs Current JSON:**
```json
{
  "resume": "input/resume.txt",
  "top": 15,
  "country": "usa",
  "fetch_limit": 10,
  "min_score": 50,
  "top_per_company": true,
  "parallel_workers": 5,
  "run_both": true,
  "selenium_only": true,
  "save_fetched": true,
  "companies": ["uber", "apple", "meta", "google", "amazon", "microsoft", "netflix"],
  "target_roles": [...],
  "target_locations": [...],
  ...
}
```

#### 2.2 Reduce Configuration Options
**Remove these unused/confusing options:**
- `run_both` (always true)
- `selenium_only` (always true)
- `save_fetched` (always true)
- `country` (use locations instead)
- `top` (redundant with fetch_limit)
- `target_roles` (use min_score instead)

---

### Phase 3: Simplify Job Scraping ğŸŒ

#### 3.1 Remove Generic Scraping
**Current:** Tries to work with ANY career page (fails often)
**New:** Portal-specific scrapers (like AIHawk)

```python
# resume_py/portals/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ base_portal.py
â”œâ”€â”€ amazon.py      # Amazon-specific logic
â”œâ”€â”€ google.py      # Google-specific logic
â”œâ”€â”€ apple.py       # Apple-specific logic
â””â”€â”€ meta.py        # Meta-specific logic
```

**Each portal has:**
```python
class AmazonPortal(BasePortal):
    def scrape_jobs(self) -> List[Job]:
        # Amazon-specific scraping
        
    def extract_description(self, url: str) -> str:
        # Amazon-specific extraction
```

#### 3.2 Simplify Job Description Fetching
**Current:** Multiple fallbacks with complex logic
**New:** Simple cascade

```python
def fetch_job_description(url: str, company: str) -> str:
    """Simple cascade: Portal-specific â†’ Generic â†’ Minimal"""
    
    # 1. Try portal-specific extractor
    portal = get_portal(company)
    if portal:
        try:
            return portal.extract_description(url)
        except:
            pass
    
    # 2. Try generic HTML extraction
    try:
        return extract_from_html(url)
    except:
        pass
    
    # 3. Return minimal description
    return f"Job at {company}: {url}"
```

---

### Phase 4: Better Error Handling ğŸ›¡ï¸

#### 4.1 Use Custom Exceptions (like AIHawk)
```python
# exceptions.py (NEW)
class JobFetchException(Exception):
    """Raised when job fetching fails"""
    pass

class ResumeGenerationException(Exception):
    """Raised when resume generation fails"""
    pass

class InsufficientJobDataException(Exception):
    """Raised when job data is incomplete"""
    pass
```

#### 4.2 Better Error Messages
**Current:**
```python
except Exception as e:
    print(f"Error: {e}")
```

**New:**
```python
except JobFetchException as e:
    logger.warning(f"Failed to fetch job from {company}: {e}")
    logger.info("Trying fallback method...")
except ResumeGenerationException as e:
    logger.error(f"Resume generation failed for {company}: {e}")
    logger.info("Skipping this job and continuing...")
```

---

### Phase 5: Add Caching ğŸ’¾

#### 5.1 Cache Generated Resumes
```python
# cache/resumes/
# amazon_software_engineer_hash123.txt

def generate_resume(job_desc: str, company: str, role: str) -> str:
    cache_key = hashlib.md5(f"{job_desc}{company}{role}".encode()).hexdigest()
    cache_file = f"cache/resumes/{company}_{role}_{cache_key}.txt"
    
    if os.path.exists(cache_file):
        logger.info(f"Using cached resume for {company} - {role}")
        return read_cache(cache_file)
    
    resume = generate_new_resume(job_desc, company, role)
    save_cache(cache_file, resume)
    return resume
```

**Benefits:**
- Faster re-runs
- Reduced API costs
- Consistent results

---

### Phase 6: Simplified Pipeline ğŸš€

#### Current Pipeline (Complex):
```
Scrape â†’ Parse HTML â†’ Create embeddings â†’ RAG retrieval â†’ 
Extract keywords â†’ Fuzzy match â†’ Filter by score â†’ 
Filter by role â†’ Filter by location â†’ Top per company â†’ 
Parallel fetch â†’ Parse again â†’ Generate resume â†’ Generate cover letter
```

#### New Pipeline (Simple):
```
Scrape â†’ Direct LLM extraction â†’ Score â†’ Filter â†’ 
Parallel generate (resume + cover letter)
```

---

## ğŸ“Š Before vs After

| Aspect | Before | After |
|--------|--------|-------|
| **Dependencies** | 13+ packages | 5-7 packages |
| **Config** | 196 line JSON | 30 line YAML |
| **Code Files** | 15+ files | 8-10 files |
| **Complexity** | High | Medium |
| **Reliability** | 60% | 90% |
| **Debug Time** | Hours | Minutes |
| **Maintenance** | High | Low |

---

## ğŸ¯ Implementation Priority

### Week 1: Critical Fixes âš¡
1. âœ… Remove embedding dependency (make optional)
2. âœ… Fix company name extraction
3. âœ… Add comprehensive logging
4. âœ… Ensure job descriptions are fetched

### Week 2: Simplification ğŸ”§
1. Convert config to YAML
2. Remove unused config options
3. Add caching for resumes
4. Better error handling

### Week 3: Portal-Specific ğŸŒ
1. Create portal base class
2. Amazon portal implementation
3. Google portal implementation
4. Test with all 7 companies

### Week 4: Polish âœ¨
1. Documentation update
2. Add unit tests
3. Performance optimization
4. GitHub Actions workflow cleanup

---

## ğŸš€ Quick Wins (Do Now!)

### 1. Remove Embedding Requirement
```python
# match.py - Line ~640
use_llm_parser = False  # Disable by default
# Only enable if embeddings are explicitly configured
```

### 2. Simplify Config
Create `config.simple.yaml`:
```yaml
resume: input/resume.txt
companies: [uber, apple, meta, google, amazon, microsoft, netflix]
min_score: 50
fetch_limit: 10
parallel_workers: 5
```

### 3. Add Debug Mode
```python
# Set DEBUG=1 for verbose logging
if os.getenv("DEBUG"):
    logging.basicConfig(level=logging.DEBUG)
```

### 4. Cache Job Descriptions
```python
# Save fetched descriptions to avoid re-fetching
cache_dir = Path("cache/jobs")
cache_dir.mkdir(exist_ok=True)
```

---

## ğŸ’¡ Expected Results

After simplification:
- âœ… 3x faster execution
- âœ… 90%+ reliability (like AIHawk)
- âœ… 50% less code
- âœ… 10x easier to debug
- âœ… Works in GitHub Actions consistently
- âœ… Generates resumes for ALL matched jobs

---

## ğŸ¬ Next Steps

1. **Accept these changes?** I can start implementing immediately
2. **Priority order?** Which simplifications do you want first?
3. **Keep or remove?** Any features you definitely want to keep?

Let me know and I'll start refactoring! ğŸš€

