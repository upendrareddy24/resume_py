name: Resume Job Matcher (Pipelines)

on:
  # schedule:
  #   - cron: '0 */6 * * *'  # every 6 hours (optional)
  workflow_dispatch:

permissions:
  contents: read

concurrency:
  group: job-matcher-pipelines
  cancel-in-progress: true

jobs:
  generate-matrix:
    runs-on: ubuntu-latest
    timeout-minutes: 5
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Detect project directory
        id: detect
        shell: bash
        run: |
          set -euo pipefail
          # Supports both layouts:
          #  - repo root IS the project (config.json present)
          #  - repo root contains a subdir (resume_py/) with the project
          if [ -f "config.json" ] && [ -f "requirements.txt" ]; then
            PROJECT_DIR="."
          elif [ -f "resume_py/config.json" ] && [ -f "resume_py/requirements.txt" ]; then
            PROJECT_DIR="resume_py"
          else
            echo "âŒ Could not detect project directory."
            echo "Repo root:"
            ls -la
            echo ""
            echo "resume_py (if exists):"
            ls -la resume_py 2>/dev/null || true
            exit 1
          fi
          echo "PROJECT_DIR=$PROJECT_DIR" >> "$GITHUB_ENV"
          echo "âœ… PROJECT_DIR=$PROJECT_DIR"

      - name: Build pipeline matrix from config.json (group by company)
        id: set-matrix
        run: |
          MATRIX_JSON="$(python3 "${PROJECT_DIR}/tools/pipelines_runtime.py" matrix --config "${PROJECT_DIR}/config.json")"
          echo "matrix=${MATRIX_JSON}" >> "$GITHUB_OUTPUT"

  match-jobs:
    needs: generate-matrix
    runs-on: ubuntu-latest
    timeout-minutes: 60
    strategy:
      fail-fast: false
      max-parallel: 4
      matrix: ${{ fromJson(needs.generate-matrix.outputs.matrix) }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Detect project directory
        shell: bash
        run: |
          set -euo pipefail
          if [ -f "config.json" ] && [ -f "requirements.txt" ]; then
            PROJECT_DIR="."
          elif [ -f "resume_py/config.json" ] && [ -f "resume_py/requirements.txt" ]; then
            PROJECT_DIR="resume_py"
          else
            echo "âŒ Could not detect project directory."
            ls -la
            exit 1
          fi
          echo "PROJECT_DIR=$PROJECT_DIR" >> "$GITHUB_ENV"
          echo "âœ… PROJECT_DIR=$PROJECT_DIR"

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: |
            requirements.txt
            resume_py/requirements.txt

      - name: Install dependencies
        run: |
          cd "${PROJECT_DIR}"
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Install Chrome (for Selenium)
        uses: browser-actions/setup-chrome@v1
        with:
          chrome-version: stable

      - name: Generate per-company pipeline config (runtime)
        run: |
          python3 "${PROJECT_DIR}/tools/pipelines_runtime.py" write-config \
            --config "${PROJECT_DIR}/config.json" \
            --company "${{ matrix.company }}" \
            --safe-name "${{ matrix.safe_name }}" \
            --out "${PROJECT_DIR}/configs/pipelines/${{ matrix.safe_name }}.json"

      - name: Normalize Selenium selectors (auto)
        run: |
          python3 "${PROJECT_DIR}/update_selenium_selectors.py" \
            --config "${PROJECT_DIR}/configs/pipelines/${{ matrix.safe_name }}.json" \
            --apply

      - name: Run matcher (per pipeline config)
        timeout-minutes: 55
        env:
          SERPAPI_KEY: ${{ secrets.SERPAPI_KEY }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          LLM_PROVIDER: "gemini"
          GEMINI_RESUME_MODEL: "gemini-2.5-flash"
          OPENAI_EMBED_MODEL: "text-embedding-3-small"
          PYTHONUNBUFFERED: "1"
        run: |
          cd "${PROJECT_DIR}"
          echo "ðŸš€ Starting job matcher (pipeline: ${{ matrix.safe_name }})..."
          echo "Company: ${{ matrix.company }}"
          echo "Config:  configs/pipelines/${{ matrix.safe_name }}.json"
          echo "â° Timeout: 55 minutes"
          echo ""
          
          # Run with verbose output and timeout (3300 seconds = 55 minutes)
          timeout 3300 python match.py --config "configs/pipelines/${{ matrix.safe_name }}.json" 2>&1 | tee "match_${{ matrix.safe_name }}.log" || {
            EXIT_CODE=$?
            if [ $EXIT_CODE -eq 124 ]; then
              echo ""
              echo "âŒ ERROR: Process timed out after 55 minutes!"
              exit 1
            else
              exit $EXIT_CODE
            fi
          }
          
          echo ""
          echo "âœ… Matcher completed for ${{ matrix.safe_name }}"

      - name: List outputs
        if: always()
        run: |
          cd "${PROJECT_DIR}"
          echo "ðŸ“ Output directory structure:"
          ls -lR output/ 2>/dev/null || echo "No output directory found"

      - name: Upload match log
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: match-log-${{ matrix.safe_name }}-${{ github.run_id }}-${{ github.run_attempt }}
          path: |
            match_${{ matrix.safe_name }}.log
            resume_py/match_${{ matrix.safe_name }}.log
          if-no-files-found: warn
          retention-days: 7

      - name: Upload output artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: output-${{ matrix.safe_name }}-${{ github.run_id }}-${{ github.run_attempt }}
          path: |
            output/
            resume_py/output/
          if-no-files-found: warn
          retention-days: 7

  aggregate-results:
    name: Aggregate outputs (one folder + one CSV)
    needs: match-jobs
    runs-on: ubuntu-latest
    if: always()
    timeout-minutes: 15
    steps:
      - name: Download all output artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: output-*-${{ github.run_id }}-${{ github.run_attempt }}
          path: combined/raw

      - name: Combine into single folder + CSV
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p combined/final
          mkdir -p combined/final/output
          
          # Copy all outputs into one folder tree (namespaced by artifact folder to avoid collisions)
          for d in combined/raw/output-*; do
            [ -d "$d" ] || continue
            name="$(basename "$d")"
            if [ -d "$d/output" ]; then
              mkdir -p "combined/final/output/${name}"
              cp -R "$d/output/." "combined/final/output/${name}/" || true
            fi
            if [ -d "$d/resume_py/output" ]; then
              mkdir -p "combined/final/output/${name}"
              cp -R "$d/resume_py/output/." "combined/final/output/${name}/" || true
            fi
          done
          
          # Merge scored_jobs.csv into one CSV with an extra "pipeline" column.
          python3 - << 'PY'
          import csv
          from pathlib import Path
          
          base = Path("combined/final/output")
          out_csv = Path("combined/final/combined_scored_jobs.csv")
          
          rows = []
          header = None
          
          for pipeline_dir in sorted(base.glob("output-*/")):
              scored = pipeline_dir / "scored_jobs.csv"
              if not scored.exists():
                  # Some runs may only write timestamped CSVs; skip if not present.
                  continue
              with scored.open("r", encoding="utf-8", newline="") as f:
                  r = csv.reader(f)
                  h = next(r, None)
                  if not h:
                      continue
                  if header is None:
                      header = ["pipeline"] + h
                  for row in r:
                      rows.append([pipeline_dir.name.replace("output-", "")] + row)
          
          if header is None:
              # Still write an empty file so the artifact is predictable
              out_csv.write_text("pipeline\n", encoding="utf-8")
          else:
              out_csv.parent.mkdir(parents=True, exist_ok=True)
              with out_csv.open("w", encoding="utf-8", newline="") as f:
                  w = csv.writer(f)
                  w.writerow(header)
                  w.writerows(rows)
          print(f"Wrote {out_csv} with {len(rows)} rows")
          PY

      - name: Upload aggregated artifact
        uses: actions/upload-artifact@v4
        with:
          name: aggregated-output-${{ github.run_id }}-${{ github.run_attempt }}
          path: combined/final/
          if-no-files-found: warn
          retention-days: 14


